{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HW_5.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN4649ZB6R2fWBYjXsccskz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/diego6289/CAP4630/blob/master/HW_5/HW_5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PKp64NdInsW_",
        "colab_type": "text"
      },
      "source": [
        "##General Concepts\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lYopZB_Pz2Nv",
        "colab_type": "text"
      },
      "source": [
        "**What is Aritificial Intelligence?**\n",
        "\n",
        "Artificial intelligence refers to the simulation of human intelligence in machines that are programmed \n",
        "to think like humans and mimic their actions. The term may also be applied to any machine that exhibits \n",
        "traits associated with a human mind such as learning and problem-solving. Artificial intelligence is based on the principle that human intelligence can be defined in a way that a machine can easily mimic it and execute tasks, from the most simple to those that are even more complex. \n",
        "\n",
        "\n",
        "**What is Machine Learning?**\n",
        "\n",
        "Machine learning is an application of artificial intelligence that provides systems the ability to \n",
        "automatically learn and improve from experience without being explicitly programmed. Machine learning focuses on \n",
        "the development of computer programs that can access data and use it learn for themselves.\n",
        "\n",
        "**What is Deep Learning?**\n",
        "\n",
        "Deep learning is an artificial intelligence function that imitates the workings of the human brain in processing \n",
        "data and creating patterns for use in decision making. Deep learning is a subset of machine learning in \n",
        "artificial intelligence that has networks capable of learning unsupervised from data that is unstructured \n",
        "or unlabeled. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "huFz0na0vw-2",
        "colab_type": "text"
      },
      "source": [
        "##Basic Concepts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZbq4i_xws84",
        "colab_type": "text"
      },
      "source": [
        "**Linear Regression:**\n",
        "\n",
        "Linear regression attempts to model the relationship between two variables by fitting a linear equation to observed data. We are essentially trying to find a linear relationship between the target and one or more predictors. The core idea is to obtain a line that best fits the data. One where the error of all data points are as small as possible. \n",
        "\n",
        "A linear regression line has an equation of the form Y = a + bX, where X is the explanatory variable and Y is the dependent variable.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BNRKHIU5kA-v",
        "colab_type": "text"
      },
      "source": [
        "**Logistic Regression:**\n",
        "\n",
        "Logistic Regression is a probablility model similar to linear regression except instead of a linear function we use a different cost function. An example would be the sigmoid function.\n",
        "Logistic Regression is used when the dependent variable(target) is categorical. For example we can determine whether email is spam(0) or not(1)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZYpotwnXkfn0",
        "colab_type": "text"
      },
      "source": [
        "**Gradient:**\n",
        "\n",
        "The gradient is closely related to a derivative. It's a vector that points in the direction of greatest increase of a function. By taking steps in that direction, we hope to reach an optimal solution.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "z7KJZ3t8e36P"
      },
      "source": [
        "**Gradient Descent:**\n",
        "\n",
        "Gradient descent is an optimization algorithm used for finding the weights or coefficients of machine learning algorithms. It works by having the model make predictions on training data and using the error on the predictions to update the model in such a way as to reduce the error. The goal of the algorithm is to find model parameters that minimize the error of the model on the training dataset. This is psuedocode that summarizes the gradient descent algorithm:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "fTYraHNbe36H",
        "colab": {}
      },
      "source": [
        "model = initialization(...)\n",
        "n_epochs = ...\n",
        "train_data = ...\n",
        "for i in n_epochs:\n",
        "\ttrain_data = shuffle(train_data)\n",
        "\tX, y = split(train_data)\n",
        "\tpredictions = predict(X, model)\n",
        "\terror = calculate_error(y, predictions)\n",
        "\tmodel = update_model(model, error)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kVr537vMvOxw",
        "colab_type": "text"
      },
      "source": [
        "##Building a Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rZqs0zggvgrc",
        "colab_type": "text"
      },
      "source": [
        "###Convolutional Neural Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ccH6geAHGbF",
        "colab_type": "text"
      },
      "source": [
        "Convolutional networks are a deep learning algorithm which can take in an input image, assign importance to various aspects/objects in the image (weights and biases) and be able to differentiate one from the other. There are three basic layers that compose a CNN.\n",
        "\n",
        "**Convolutional Layer**:\n",
        "\n",
        "Convolution is the first layer to extract features from an input image. Colvolution preserves the relationship between pixels by learning image features using small squares of input data. It takes two inputs such as image matrix and a filter or kernel.\n",
        "\n",
        "**Activation Function:**\n",
        "It is used to determine the output of a neural network (like 1/0). One activation function we used was the Sigmoid. *Example below.*\n",
        "\n",
        "**Strides and Padding**:\n",
        "\n",
        "Stride is the number of pixels shifts over the input matrix. Padding is the number of pixels added to an image when it is being process by the kernel of a CNN. Stride and padding work together to allow for a minimized reduction of size in the output layer.\n",
        "\n",
        "**Pooling Layer**:\n",
        "\n",
        "It is responsible for reducing the spatial size of the convoluted feature. Pooling combines the output of neuron cluster at one layer into a single neuron in the next layer.\n",
        "One method of doing this is Max Pooling, where we use the maximum value of each cluster of neurons at the prior layer. It helps reduce overfitting and aids efficiency.\n",
        "\n",
        "**Fully Connected Layer:**\n",
        "\n",
        "Fully Connected Layer is simply, feed forward neural networks. The input to the fully connected layer is the output from the final Pooling or Convolutional Layer, which is flattened and then fed into the fully connected layer.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eYem8erwPrYT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sigmoid_function(z):             # Example of Activation Function\n",
        "  return 1 / (1 + np.exp(-1 * z))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rsAGNm3gv-9C",
        "colab_type": "text"
      },
      "source": [
        "##Comping a Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "narINMtjW6bm",
        "colab_type": "text"
      },
      "source": [
        "**Optimizers:**\n",
        "\n",
        "Optimizers tie together the loss function and model parameters by updating the model in response to the output of the loss function. The loss function is the guide to the terrain, telling the optimizer when it’s moving in the right or wrong direction.\n",
        "\n",
        "**Learning Rate:**\n",
        "\n",
        "Learning rate ensures that we change our weights at the right pace, not making any changes that are too big or too small.\n",
        "Changing our weights too fast by adding or subtracting too much can hinder our ability to minimize the loss function. We don’t want to make a jump so large that we skip over the optimal value for a given weight. Similarly, we don’t want to take steps that are too small, because then we might never end up with the right values for our weight."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F0FWZj_xsEsp",
        "colab_type": "text"
      },
      "source": [
        "**Loss Function:**\n",
        "\n",
        "The loss function is a method of evaluating how well the algorithm models the dataset. If the predictions are totally off, the loss function will output a higher number. If they’re good, it’ll output a lower number. The loss function will let us know how the model is doing. This is an example of the loss function from HW 3."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e6UvH8l4sFg4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def loss_computation(A, B):\n",
        "  partial = m * .80\n",
        "  loss_val = 0\n",
        "  for data_val, label in zip(A, B):\n",
        "    predictor = np.dot(np.reshape(weights_manually_calculated, (2, )), data_val) + bias_term\n",
        "    bce = entropy_func(label, sigmoid_function(predictor))\n",
        "    loss_val += bce\n",
        "  loss_val /= (partial)\n",
        "  return loss_val"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uL6DJGsEwIZN",
        "colab_type": "text"
      },
      "source": [
        "##Training a Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FAkxQ9Hnnwg5",
        "colab_type": "text"
      },
      "source": [
        "We use the .fit() method to train our model. This is an example used in the previous homework."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nJbItBrGP5jc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# importing tensorflow package \n",
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "\n",
        "def keras_model():\n",
        "  glm_model = tf.keras.models.Sequential()\n",
        "\n",
        "  # adding layers and activation function\n",
        "  glm_model.add(tf.keras.layers.Dense(1, activation='sigmoid', input_shape = (2, )))\n",
        "\n",
        "  glm_model.compile(optimizer = tf.keras.optimizers.RMSprop(lr = 0.001), loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
        "  return glm_model\n",
        "\n",
        "  # setting the number of epochs\n",
        "epochs_count = 300\n",
        "\n",
        "my_keras_model = keras_model()\n",
        "\n",
        "history_of_model = my_keras_model.fit(training_set, training_set_label, epochs = epochs_count, batch_size = 512, validation_data = (testing_set, testing_set_label))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LqvdSn5twKpY",
        "colab_type": "text"
      },
      "source": [
        "**Overfitting:**\n",
        "\n",
        "Overfitting happens when we train a model with too much data. When a model gets trained with so much data, it starts learning from the noise and inaccurate data entries in our data set. Then the model does not categorize the data correctly, because of too much of details and noise. To avoid overfitting we can use cross-validation.\n",
        "\n",
        "**Underfitting:**\n",
        "\n",
        "Underfitting occurs when a model cannot capture the underlying trend of the data. Underfitting destroys the accuracy of the model. It can be avoided by using more data and also by reducing features."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mDkRI3TQrQZ7",
        "colab_type": "text"
      },
      "source": [
        "##Finetuning a Pretrained Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Th7u6mkmrYp3",
        "colab_type": "text"
      },
      "source": [
        "**Finetuning a pretrained model:**\n",
        "\n",
        "Fine-tuning a network is basically optimising the parameters and hyperparameters of an already trained network to adapt it to the new task. We only do it if our current dataset is not drastically different from the dataset on which the network was trained. Another concern to watch out for is that if our dataset is small, fine-tuning can lead to overfitting, especially in the case of VGG since the last few layers are fully connected layers. Now, if we ensure that both the cases are not applicable we can proceed to fine tune the network. \n",
        "\n",
        "First, we try to truncate the last layer (softmax in classification problems) of the pre-trained network and replace it with the softmax layer applicable to our data. For instance, if pretrained model had softmax with 100 categories but we are working with 5 categories. In addition, we can also use different classifiers which might be suitable to our application and evaluate which one fits best.\n",
        "\n",
        "Then, we freeze the weights of the initial layers of the pre-trained model and train only the last few layers because the first few layers capture the general/universal features (like edges and curves in a dog/cat problem) of our classification problem.\n",
        "\n",
        "We use a smaller learning rate for training because we don't to completely undo the pre-training of weights, just want to tune them and not distort them too fast or too much."
      ]
    }
  ]
}